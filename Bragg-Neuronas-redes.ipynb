{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Librerías Necesarias\n",
    "Importar las librerías requeridas como numpy, pandas, tensorflow, matplotlib, seaborn, entre otras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas tensorflow keras keras-tuner scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Importar las librerías necesarias\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Importar las librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_tuner.tuners import Hyperband\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from keras import callbacks\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar y Preprocesar los Datos\n",
    "Cargar el archivo CSV, mezclar los datos y separar las características (X) y los objetivos (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y Preprocesar los Datos\n",
    "\n",
    "# Cargar los datos desde el archivo CSV\n",
    "dataset = pd.read_csv('original_noise10_Train_set')\n",
    "\n",
    "# Reiniciar los índices del DataFrame\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "# Mezclar los datos aleatoriamente\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Separar las características (X) y los objetivos (Y)\n",
    "X = dataset.iloc[:, 3:-2].values  # Seleccionar columnas de características\n",
    "Y = dataset.iloc[:, -2:].values   # Seleccionar columnas de objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir el Modelo de Red Neuronal\n",
    "Utilizar la clase Sequential de Keras para definir una red neuronal con capas densas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el Modelo de Red Neuronal\n",
    "model = Sequential()\n",
    "\n",
    "# Agregar la primera capa densa con 64 neuronas y función de activación ReLU\n",
    "model.add(Dense(64, input_dim=X.shape[1], activation='relu'))\n",
    "\n",
    "# Agregar una segunda capa densa con 32 neuronas y función de activación ReLU\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Agregar la capa de salida con 2 neuronas (una por cada objetivo) y función de activación lineal\n",
    "model.add(Dense(Y.shape[1], activation='linear'))\n",
    "\n",
    "# Compilar el modelo con el optimizador Adam y la métrica de error cuadrático medio\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Mostrar un resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilar y Entrenar el Modelo\n",
    "Configurar el optimizador, la función de pérdida y las métricas. Entrenar el modelo utilizando los datos preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar y Entrenar el Modelo\n",
    "\n",
    "# Configurar EarlyStopping para detener el entrenamiento si no hay mejora en el conjunto de validación\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento y validación\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Graficar la pérdida y la métrica de entrenamiento y validación\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n",
    "plt.title('Pérdida durante el Entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['mae'], label='MAE de Entrenamiento')\n",
    "plt.plot(history.history['val_mae'], label='MAE de Validación')\n",
    "plt.title('MAE durante el Entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluar el Modelo\n",
    "Evaluar el rendimiento del modelo utilizando métricas relevantes y validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el Modelo\n",
    "\n",
    "# Configurar validación cruzada con 5 particiones\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Listas para almacenar las métricas de cada partición\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Realizar validación cruzada\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    # Entrenar el modelo en los datos de entrenamiento\n",
    "    model.fit(X_train, Y_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Evaluar el modelo en los datos de prueba\n",
    "    scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    mse_scores.append(scores[0])  # Pérdida (MSE)\n",
    "    mae_scores.append(scores[1])  # MAE\n",
    "\n",
    "# Calcular las métricas promedio\n",
    "mean_mse = np.mean(mse_scores)\n",
    "mean_mae = np.mean(mae_scores)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"MSE promedio en validación cruzada: {mean_mse:.4f}\")\n",
    "print(f\"MAE promedio en validación cruzada: {mean_mae:.4f}\")\n",
    "\n",
    "# Graficar las métricas de validación cruzada\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, 6), mse_scores, label='MSE por partición', color='skyblue')\n",
    "plt.axhline(mean_mse, color='red', linestyle='--', label='MSE Promedio')\n",
    "plt.title('MSE en Validación Cruzada')\n",
    "plt.xlabel('Partición')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, 6), mae_scores, label='MAE por partición', color='lightgreen')\n",
    "plt.axhline(mean_mae, color='red', linestyle='--', label='MAE Promedio')\n",
    "plt.title('MAE en Validación Cruzada')\n",
    "plt.xlabel('Partición')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizar Resultados\n",
    "Generar gráficos para visualizar el rendimiento del modelo, como la pérdida y la precisión durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.11' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Visualizar Resultados\n",
    "\n",
    "# Graficar la pérdida y la métrica de entrenamiento y validación\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n",
    "plt.title('Pérdida durante el Entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['mae'], label='MAE de Entrenamiento')\n",
    "plt.plot(history.history['val_mae'], label='MAE de Validación')\n",
    "plt.title('MAE durante el Entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Graficar las métricas de validación cruzada\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, 6), mse_scores, label='MSE por partición', color='skyblue')\n",
    "plt.axhline(mean_mse, color='red', linestyle='--', label='MSE Promedio')\n",
    "plt.title('MSE en Validación Cruzada')\n",
    "plt.xlabel('Partición')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, 6), mae_scores, label='MAE por partición', color='lightgreen')\n",
    "plt.axhline(mean_mae, color='red', linestyle='--', label='MAE Promedio')\n",
    "plt.title('MAE en Validación Cruzada')\n",
    "plt.xlabel('Partición')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
